{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03454df-1f49-41a2-b06a-ab35a1abc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████| 8/8 [05:42<00:00, 42.75s/it]\n",
      "Loading checkpoint shards:  50%|█████████████████████████████████▌                                 | 4/8 [00:44<00:44, 11.18s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Make sure you are logged in if using gated models like Llama 2\n",
    "# Run in terminal: huggingface-cli login\n",
    "\n",
    "# Use the correct Hugging Face model ID\n",
    "# model_id = \"meta-llama/Llama-2-13b-chat-hf\" \n",
    "model_id = \"Qwen/Qwen1.5-14B-Chat\" \n",
    "# model_id = \"google/gemma-1.1-9b-it\" \n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "\n",
    "try:\n",
    "    # Trust remote code often needed for models like Qwen\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters()) \n",
    "    # Or use built-in (might only count trainable by default sometimes, but usually gives total)\n",
    "    # total_params = model.num_parameters() \n",
    "    print(f\"Model: {model_id}\")\n",
    "    print(f\"Total Parameters: {total_params:,}\") \n",
    "except Exception as e:\n",
    "    print(f\"Could not load model {model_id}: {e}\")\n",
    "    print(\"Parameter count might be available on the Hugging Face Hub model card.\")\n",
    "\n",
    "# Note: This downloads the entire model, which can be large (tens of GB).\n",
    "# Checking the Hub website model card is much faster for just the parameter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774545f-834a-46bc-abbd-1635f0e16d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Choose ONE model ID ---\n",
    "# model_id = \"meta-llama/Llama-2-13b-chat-hf\" \n",
    "model_id = \"Qwen/Qwen1.5-14B-Chat\" \n",
    "# model_id = \"google/gemma-1.1-9b-it\" \n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "# ---\n",
    "\n",
    "print(f\"--- Analysing Model ID: {model_id} ---\")\n",
    "\n",
    "# Make sure you are logged in if using gated models like Llama 2\n",
    "# Run in terminal: huggingface-cli login\n",
    "\n",
    "try:\n",
    "    # Use trust_remote_code=True for models like Qwen or others that require it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # OPTION 1: Check the tokenizer's chat_template attribute (Jinja format)\n",
    "    print(\"\\nTokenizer's Stored Chat Template (Jinja format):\")\n",
    "    print(tokenizer.chat_template)\n",
    "\n",
    "    # OPTION 2: Apply the template to a sample conversation (often clearest)\n",
    "    print(\"\\nApplying template to a sample conversation:\")\n",
    "    sample_conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise pirate bot.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Arrr, I be fine, matey!\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
    "    ]\n",
    "\n",
    "    # Apply template WITHOUT tokenizing to see the raw string\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        sample_conversation, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True # Adds the prompt for the next assistant response\n",
    "    )\n",
    "    print(formatted_prompt)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not process tokenizer for {model_id}: {e}\")\n",
    "    print(\"Check the model name, your internet connection, and if you need to log in (huggingface-cli login).\")\n",
    "\n",
    "print(\"-\" * (len(model_id) + 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec0081c-be0d-4e4f-b3c9-ef21376a658b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_param_count\u001b[39m(model_id):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model2size.get(model_id, \u001b[33m\"\u001b[39m\u001b[33mUnknown model or size not listed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_param_count\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistralai/Mistral-7B-Instruct-v0.3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Output: 7250000000\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_param_count\u001b[39m\u001b[34m(model_id)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_param_count\u001b[39m(model_id):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel2size\u001b[49m.get(model_id, \u001b[33m\"\u001b[39m\u001b[33mUnknown model or size not listed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model2size' is not defined"
     ]
    }
   ],
   "source": [
    "def get_param_count(model_id):\n",
    "    return model2size.get(model_id, \"Unknown model or size not listed.\")\n",
    "\n",
    "print(get_param_count(\"mistralai/Mistral-7B-Instruct-v0.3\"))\n",
    "# Output: 7250000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa127c7-18de-41d6-8bc6-d030921d1771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
